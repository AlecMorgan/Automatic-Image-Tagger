{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping\n",
    "\n",
    "### Using Selenium, this notebook initializes a zombie web browser to scrape images from Instagram's latest posts. First it navigates to a hashtag's page and grabs links to a certain number of posts, then it visits each post and grabs its image, its hashtags, and a few other pieces of information such as a direct link to the post. All of this data is then saved directly into the `data` and `metadata` folders. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumPy versions below 1.17 may be incompatible with some other \n",
    "# packages, so you may need to replace your current version with \n",
    "# an earlier one in order to run this notebook as-is. \n",
    "# !pip uninstall numpy --yes\n",
    "# !pip install \"numpy<1.17\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from selenium.webdriver import Chrome, Firefox\n",
    "from functions import scrape_data\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **If you want to scrape your own hashtags,** simply add them to the list, choose how many you want to be scraped, and run the remaining cells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLE:\n",
    "# hashtags = [\"travel\", \"food\", \"animals\", \"selfie\", \"cars\", \"fitness\", \"babies\", \"wedding\", \"nature\", \"architecture\"]\n",
    "\n",
    "# Your own hashtags here:\n",
    "hashtags = []\n",
    "\n",
    "# How many hashtags to scrape:\n",
    "num_to_scrape = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder 'data' already exists.\n",
      "Folder 'metadata' already exists.\n"
     ]
    }
   ],
   "source": [
    "# Make sure our data and metadata folders exist before we start scraping\n",
    "folder_names = [\"data\", \"metadata\"]\n",
    "for folder_name in folder_names:\n",
    "    try:\n",
    "        os.mkdir(folder_name)\n",
    "    except OSError:\n",
    "        print(f\"Folder '{folder_name}' already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for hashtag in hashtags:\n",
    "    # \"delay\" is how long to wait between grabbing each image, to avoid being \n",
    "    # blocked by Instagram. If delay=5 for example, then the browser will \n",
    "    # randomly wait between 0 to 5 seconds before grabbing each new image.\n",
    "    new_hashtag_metadata = scrape_data(hashtag, num_to_scrape, delay=5)\n",
    "    \n",
    "    if os.path.exists(f\"metadata/{hashtag}.json\"):\n",
    "        # We already have metadata for this hashtag, add to it\n",
    "        with open(f\"metadata/{hashtag}.json\", \"r\") as f:\n",
    "            hashtag_metadata = json.load(f)\n",
    "            hashtag_metadata += new_hashtag_metadata\n",
    "    else:\n",
    "        # We don't have metadata for this hashtag yet, initialize it\n",
    "        hashtag_metadata = new_hashtag_metadata\n",
    "\n",
    "    with open(f\"metadata/{hashtag}.json\", \"w\") as f:\n",
    "        json.dump(hashtag_metadata, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You can use `pd.read_json` to import hashtag data again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# travel_df = pd.read_json(\"metadata/travel.json\")\n",
    "# travel_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optionally you can also use this scaffolding for uploading scraped images to an S3 bucket, although you will of course need to set up your own S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import boto3\n",
    "\n",
    "# s3 = boto3.resource(\"s3\")\n",
    "\n",
    "# hashtags_to_upload = [\"foo\", \"bar\"]\n",
    "# for hashtag in hashtags_to_upload:\n",
    "#     for img in hashtag: \n",
    "#         source = f\"data/{img[\"image_local_name\"]}\"\n",
    "#         bucket = f\"instagram-images-mod4\"\n",
    "#         destination = f\"{img[\"search_hashtag\"]}/{img[\"image_local_name\"]}\"\n",
    "#         s3.meta.client.upload_file(source, bucket, destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
